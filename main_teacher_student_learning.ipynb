{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dill\n",
    "\n",
    "from constant import *\n",
    "from environment import *\n",
    "from behavior import *\n",
    "from utils import utils\n",
    "from utils.chronometer import Chronometer\n",
    "\n",
    "sys.path.append('../Nurture/server/notification/')\n",
    "from nurture.learning.agents import *\n",
    "from nurture.learning.state import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior/data/2.txt\n",
      "behavior/data/4.txt\n",
      "behavior/data/5.txt\n",
      "behavior/data/6.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No record for day=0, location=0, activity=2, notification=0\n",
      "No record for day=0, location=0, activity=2, notification=1\n",
      "No record for day=0, location=0, activity=3, notification=0\n",
      "No record for day=0, location=0, activity=3, notification=1\n",
      "No record for day=0, location=0, activity=4, notification=0\n",
      "No record for day=0, location=0, activity=4, notification=1\n",
      "No record for day=0, location=1, activity=2, notification=0\n",
      "No record for day=0, location=1, activity=2, notification=1\n",
      "No record for day=0, location=1, activity=3, notification=0\n",
      "No record for day=0, location=1, activity=3, notification=1\n",
      "No record for day=0, location=1, activity=4, notification=0\n",
      "No record for day=0, location=1, activity=4, notification=1\n",
      "No record for day=1, location=0, activity=2, notification=0\n",
      "No record for day=1, location=0, activity=2, notification=1\n",
      "No record for day=1, location=0, activity=3, notification=0\n",
      "No record for day=1, location=0, activity=3, notification=1\n",
      "No record for day=1, location=0, activity=4, notification=0\n",
      "No record for day=1, location=0, activity=4, notification=1\n",
      "No record for day=1, location=1, activity=2, notification=0\n",
      "No record for day=1, location=1, activity=2, notification=1\n",
      "No record for day=1, location=1, activity=3, notification=0\n",
      "No record for day=1, location=1, activity=3, notification=1\n",
      "No record for day=1, location=1, activity=4, notification=0\n",
      "No record for day=1, location=1, activity=4, notification=1\n",
      "WARNING: No records for 24 states. The behavior will be random.\n"
     ]
    }
   ],
   "source": [
    "# environment setup\n",
    "rewardCriteria = {\n",
    "        ANSWER_NOTIFICATION_ACCEPT: 1,\n",
    "        ANSWER_NOTIFICATION_IGNORE: 0,\n",
    "        ANSWER_NOTIFICATION_DISMISS: -5,\n",
    "}\n",
    "environment = MTurkSurveyUser(\n",
    "    filePaths=[\n",
    "            'survey/ver2_mturk/results/01_1st_Batch_3137574_batch_results.csv',\n",
    "            'survey/ver2_mturk/results/02_Batch_3148398_batch_results.csv',\n",
    "            'survey/ver2_mturk/results/03_Batch_3149214_batch_results.csv',\n",
    "    ],\n",
    "    filterFunc=(lambda r: ord(r['rawWorkerID'][-1]) % 3 == 2),\n",
    ")\n",
    "\n",
    "behavior = ExtraSensoryBehavior([\n",
    "    'behavior/data/2.txt',\n",
    "    'behavior/data/4.txt',\n",
    "    'behavior/data/5.txt',\n",
    "    'behavior/data/6.txt',\n",
    "])\n",
    "simulationLengthDay = 140\n",
    "stepWidthMinutes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total records: 36270\n",
      "# of `nan` in location: 8512\n",
      "# of `nan` in Activity: 18430\n",
      "Location:\n",
      "    home: 19375\n",
      "    work: 8155\n",
      "  others: 8740\n",
      "Activity:\n",
      "  stationary: 32565\n",
      "     walking: 1885\n",
      "     running: 118\n",
      "     driving: 1204\n",
      "   commuting: 498\n"
     ]
    }
   ],
   "source": [
    "behavior.printSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation setup\n",
    "chronometer = Chronometer(skipFunc=(lambda hour, _m, _d: hour < 10 or hour >= 22))\n",
    "\n",
    "lastNotificationMinute = 0\n",
    "lastNotificationHour = 0\n",
    "lastNotificationNumDays = 0\n",
    "\n",
    "# statistics\n",
    "simulationResults = []\n",
    "totalReward = 0.\n",
    "numSteps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def _get_time_of_day(currentHour, currentMinute):\n",
    "    return currentHour / 24. + currentMinute / 24. / 60.\n",
    "\n",
    "def _get_day_of_week(currentDay, currentHour, currentMinute):\n",
    "    return currentDay / 7. + currentHour / 7. / 24. + currentMinute / 7. / 24. / 60.\n",
    "\n",
    "def _get_motion(original_activity):\n",
    "    mapping = {\n",
    "        utils.STATE_ACTIVITY_STATIONARY: State.MOTION_STATIONARY,\n",
    "        utils.STATE_ACTIVITY_WALKING: State.MOTION_WALKING,\n",
    "        utils.STATE_ACTIVITY_RUNNING: State.MOTION_RUNNING,\n",
    "        utils.STATE_ACTIVITY_DRIVING: State.MOTION_DRIVING,\n",
    "        utils.STATE_ACTIVITY_COMMUTE: State.MOTION_DRIVING,\n",
    "    }\n",
    "    return mapping[original_activity]\n",
    "\n",
    "def _printResults(results):\n",
    "    notificationEvents = [r for r in results if r['decision']]\n",
    "    numNotifications = len(notificationEvents)\n",
    "    numAcceptedNotis = len([r for r in notificationEvents if r['reward'] > 0])\n",
    "    numDismissedNotis = len([r for r in notificationEvents if r['reward'] < 0])\n",
    "\n",
    "    answerRate = numAcceptedNotis / numNotifications if numNotifications > 0 else 0.\n",
    "    dismissRate = numDismissedNotis / numNotifications if numNotifications > 0 else 0.\n",
    "    numActionedNotis = numAcceptedNotis + numDismissedNotis\n",
    "    responseRate = numAcceptedNotis / numActionedNotis if numActionedNotis > 0 else 0.\n",
    "\n",
    "    totalReward = sum([r['reward'] for r in results])\n",
    "\n",
    "    expectedNumDeliveredNotifications = sum([r['probOfAnswering'] for r in results])\n",
    "    deltaDays = results[-1]['context']['numDaysPassed'] - results[0]['context']['numDaysPassed'] + 1\n",
    "\n",
    "    print(\"  reward=%f / step=%d (%f)\" % (totalReward, len(results), totalReward / len(results)))\n",
    "    print(\"  %d notifications have been sent (%.1f / day):\" % (numNotifications, numNotifications / deltaDays))\n",
    "    print(\"    - %d are answered (%.2f%%)\"  % (numAcceptedNotis, answerRate * 100.))\n",
    "    print(\"    - %d are dismissed (%.2f%%)\"  % (numDismissedNotis, dismissRate * 100.))\n",
    "    print(\"    - response rate: %.2f%%\"  % (responseRate * 100.))\n",
    "    print(\"  Expectation of total delivered notifications is %.2f\" % expectedNumDeliveredNotifications)\n",
    "\n",
    "def _filterByWeek(results, week):\n",
    "    startDay = week * 7\n",
    "    endDay = startDay + 7\n",
    "    return [r for r in results\n",
    "            if startDay <= r['context']['numDaysPassed'] < endDay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.STATE_ACTIVITY_COMMUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDaysPassed, currentHour, currentMinute, currentDay = chronometer.forward(stepWidthMinutes)\n",
    "toBePrintedWeek = 0\n",
    "agent = TensorForceDQNAgent()\n",
    "agent.agent = DQNAgent(\n",
    "        states=dict(type='float', shape=(15,)),\n",
    "        actions=dict(type='int', num_actions=2),\n",
    "        network=[\n",
    "            dict(type='dense', size=20),\n",
    "            dict(type='dense', size=20)\n",
    "        ],\n",
    "        batched_observe=False,\n",
    "        actions_exploration={\n",
    "            'type': 'epsilon_decay',\n",
    "            'initial_epsilon': 0.3,\n",
    "            'final_epsilon': 0.05,\n",
    "            'timesteps': 80000,\n",
    "        },\n",
    ")\n",
    "#agent = QLearningAgent()\n",
    "\n",
    "while numDaysPassed < simulationLengthDay:\n",
    "    # get environment info (user context)\n",
    "    lastNotificationTime = utils.getDeltaMinutes(\n",
    "            numDaysPassed, currentHour, currentMinute,\n",
    "            lastNotificationNumDays, lastNotificationHour, lastNotificationMinute,\n",
    "    )\n",
    "    #stateLastNotification = utils.getLastNotificationState(lastNotificationTime)\n",
    "    stateLocation, stateActivity = behavior.getLocationActivity(\n",
    "            currentHour, currentMinute, currentDay)\n",
    "    probAnsweringNotification, probIgnoringNotification, probDismissingNotification = (\n",
    "            environment.getResponseDistribution(\n",
    "                currentHour, currentMinute, currentDay,\n",
    "                stateLocation, stateActivity, lastNotificationTime,\n",
    "            )\n",
    "    )\n",
    "    probAnsweringNotification, probIgnoringNotification, probDismissingNotification = utils.normalize(\n",
    "            probAnsweringNotification, probIgnoringNotification, probDismissingNotification)\n",
    "\n",
    "    # prepare observations\n",
    "    state = State(\n",
    "        timeOfDay=_get_time_of_day(currentHour, currentMinute),\n",
    "        dayOfWeek=_get_day_of_week(currentDay, currentHour, currentMinute),\n",
    "        motion=_get_motion(stateActivity),\n",
    "        location=stateLocation,\n",
    "        notificationTimeElapsed=lastNotificationTime,\n",
    "        ringerMode=np.random.choice(a=State.allRingerModeValues()),\n",
    "        screenStatus=np.random.choice(a=State.allScreenStatusValues()),\n",
    "    )\n",
    "    \n",
    "    # small hack - some agent keeps track of time\n",
    "    try:\n",
    "        agent.last_notification_time -= datetime.timedelta(minutes=stepWidthMinutes)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get action\n",
    "    sendNotification = agent.get_action(state)\n",
    "\n",
    "    # calculate reward\n",
    "    if not sendNotification:\n",
    "        reward = 0\n",
    "    else:\n",
    "        userReaction = np.random.choice(\n",
    "                a=[ANSWER_NOTIFICATION_ACCEPT, ANSWER_NOTIFICATION_IGNORE, ANSWER_NOTIFICATION_DISMISS],\n",
    "                p=[probAnsweringNotification, probIgnoringNotification, probDismissingNotification],\n",
    "        )\n",
    "        reward = rewardCriteria[userReaction]\n",
    "        lastNotificationNumDays = numDaysPassed\n",
    "        lastNotificationHour = currentHour\n",
    "        lastNotificationMinute = currentMinute\n",
    "    agent.feed_reward(reward)\n",
    "\n",
    "    # log this session\n",
    "    simulationResults.append({\n",
    "            'context': {\n",
    "                'numDaysPassed': numDaysPassed,\n",
    "                'hour': currentHour,\n",
    "                'minute': currentMinute,\n",
    "                'day': currentDay,\n",
    "                'location': stateLocation,\n",
    "                'activity': stateActivity,\n",
    "                'lastNotification': lastNotificationTime,\n",
    "            },\n",
    "            'probOfAnswering': probAnsweringNotification,\n",
    "            'probOfIgnoring': probIgnoringNotification,\n",
    "            'probOfDismissing': probDismissingNotification,\n",
    "            'decision': sendNotification,\n",
    "            'reward': reward,\n",
    "    })\n",
    "\n",
    "    # get the next decision time point\n",
    "    numDaysPassed, currentHour, currentMinute, currentDay = chronometer.forward(stepWidthMinutes)\n",
    "    \n",
    "    # print current state\n",
    "    currentWeek = numDaysPassed // 7\n",
    "    if currentWeek > toBePrintedWeek:\n",
    "        print()\n",
    "        print(\"===== end of week %d ====\" % toBePrintedWeek)\n",
    "        _printResults(_filterByWeek(simulationResults, toBePrintedWeek))\n",
    "        toBePrintedWeek = currentWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== end of week 0 ====\n",
      "  reward=-370.000000 / step=5040 (-0.073413)\n",
      "  463 notifications have been sent (66.1 / day):\n",
      "    - 195 are answered (42.12%)\n",
      "    - 113 are dismissed (24.41%)\n",
      "    - response rate: 63.31%\n",
      "  Expectation of total delivered notifications is 1949.20\n",
      "\n",
      "===== end of week 1 ====\n",
      "  reward=-257.000000 / step=5040 (-0.050992)\n",
      "  397 notifications have been sent (56.7 / day):\n",
      "    - 178 are answered (44.84%)\n",
      "    - 87 are dismissed (21.91%)\n",
      "    - response rate: 67.17%\n",
      "  Expectation of total delivered notifications is 1952.30\n",
      "\n",
      "===== end of week 2 ====\n",
      "  reward=-208.000000 / step=5040 (-0.041270)\n",
      "  277 notifications have been sent (39.6 / day):\n",
      "    - 112 are answered (40.43%)\n",
      "    - 64 are dismissed (23.10%)\n",
      "    - response rate: 63.64%\n",
      "  Expectation of total delivered notifications is 2042.30\n",
      "\n",
      "===== end of week 3 ====\n",
      "  reward=-250.000000 / step=5040 (-0.049603)\n",
      "  278 notifications have been sent (39.7 / day):\n",
      "    - 110 are answered (39.57%)\n",
      "    - 72 are dismissed (25.90%)\n",
      "    - response rate: 60.44%\n",
      "  Expectation of total delivered notifications is 2014.30\n",
      "\n",
      "===== end of week 4 ====\n",
      "  reward=-90.000000 / step=5040 (-0.017857)\n",
      "  193 notifications have been sent (27.6 / day):\n",
      "    - 95 are answered (49.22%)\n",
      "    - 37 are dismissed (19.17%)\n",
      "    - response rate: 71.97%\n",
      "  Expectation of total delivered notifications is 2067.30\n",
      "\n",
      "===== end of week 5 ====\n",
      "  reward=-155.000000 / step=5040 (-0.030754)\n",
      "  165 notifications have been sent (23.6 / day):\n",
      "    - 60 are answered (36.36%)\n",
      "    - 43 are dismissed (26.06%)\n",
      "    - response rate: 58.25%\n",
      "  Expectation of total delivered notifications is 2055.30\n",
      "\n",
      "===== end of week 6 ====\n",
      "  reward=-161.000000 / step=5040 (-0.031944)\n",
      "  178 notifications have been sent (25.4 / day):\n",
      "    - 79 are answered (44.38%)\n",
      "    - 48 are dismissed (26.97%)\n",
      "    - response rate: 62.20%\n",
      "  Expectation of total delivered notifications is 2117.30\n",
      "\n",
      "===== end of week 7 ====\n",
      "  reward=-119.000000 / step=5040 (-0.023611)\n",
      "  129 notifications have been sent (18.4 / day):\n",
      "    - 51 are answered (39.53%)\n",
      "    - 34 are dismissed (26.36%)\n",
      "    - response rate: 60.00%\n",
      "  Expectation of total delivered notifications is 2271.30\n",
      "\n",
      "===== end of week 8 ====\n",
      "  reward=-88.000000 / step=5040 (-0.017460)\n",
      "  138 notifications have been sent (19.7 / day):\n",
      "    - 67 are answered (48.55%)\n",
      "    - 31 are dismissed (22.46%)\n",
      "    - response rate: 68.37%\n",
      "  Expectation of total delivered notifications is 2158.30\n",
      "\n",
      "===== end of week 9 ====\n",
      "  reward=-46.000000 / step=5040 (-0.009127)\n",
      "  127 notifications have been sent (18.1 / day):\n",
      "    - 64 are answered (50.39%)\n",
      "    - 22 are dismissed (17.32%)\n",
      "    - response rate: 74.42%\n",
      "  Expectation of total delivered notifications is 2252.30\n",
      "\n",
      "===== end of week 10 ====\n",
      "  reward=-61.000000 / step=5040 (-0.012103)\n",
      "  106 notifications have been sent (15.1 / day):\n",
      "    - 49 are answered (46.23%)\n",
      "    - 22 are dismissed (20.75%)\n",
      "    - response rate: 69.01%\n",
      "  Expectation of total delivered notifications is 2162.30\n",
      "\n",
      "===== end of week 11 ====\n",
      "  reward=-50.000000 / step=5040 (-0.009921)\n",
      "  106 notifications have been sent (15.1 / day):\n",
      "    - 55 are answered (51.89%)\n",
      "    - 21 are dismissed (19.81%)\n",
      "    - response rate: 72.37%\n",
      "  Expectation of total delivered notifications is 2281.30\n",
      "\n",
      "===== end of week 12 ====\n",
      "  reward=-103.000000 / step=5040 (-0.020437)\n",
      "  107 notifications have been sent (15.3 / day):\n",
      "    - 42 are answered (39.25%)\n",
      "    - 29 are dismissed (27.10%)\n",
      "    - response rate: 59.15%\n",
      "  Expectation of total delivered notifications is 2232.30\n",
      "\n",
      "===== end of week 13 ====\n",
      "  reward=-93.000000 / step=5040 (-0.018452)\n",
      "  131 notifications have been sent (18.7 / day):\n",
      "    - 52 are answered (39.69%)\n",
      "    - 29 are dismissed (22.14%)\n",
      "    - response rate: 64.20%\n",
      "  Expectation of total delivered notifications is 2214.30\n",
      "\n",
      "===== end of week 14 ====\n",
      "  reward=-35.000000 / step=5040 (-0.006944)\n",
      "  103 notifications have been sent (14.7 / day):\n",
      "    - 45 are answered (43.69%)\n",
      "    - 16 are dismissed (15.53%)\n",
      "    - response rate: 73.77%\n",
      "  Expectation of total delivered notifications is 2337.30\n",
      "\n",
      "===== end of week 15 ====\n",
      "  reward=-101.000000 / step=5040 (-0.020040)\n",
      "  132 notifications have been sent (18.9 / day):\n",
      "    - 54 are answered (40.91%)\n",
      "    - 31 are dismissed (23.48%)\n",
      "    - response rate: 63.53%\n",
      "  Expectation of total delivered notifications is 2177.30\n",
      "\n",
      "===== end of week 16 ====\n",
      "  reward=-87.000000 / step=5040 (-0.017262)\n",
      "  102 notifications have been sent (14.6 / day):\n",
      "    - 43 are answered (42.16%)\n",
      "    - 26 are dismissed (25.49%)\n",
      "    - response rate: 62.32%\n",
      "  Expectation of total delivered notifications is 2269.30\n",
      "\n",
      "===== end of week 17 ====\n",
      "  reward=-136.000000 / step=5040 (-0.026984)\n",
      "  132 notifications have been sent (18.9 / day):\n",
      "    - 54 are answered (40.91%)\n",
      "    - 38 are dismissed (28.79%)\n",
      "    - response rate: 58.70%\n",
      "  Expectation of total delivered notifications is 2110.30\n",
      "\n",
      "===== end of week 18 ====\n",
      "  reward=-53.000000 / step=5040 (-0.010516)\n",
      "  109 notifications have been sent (15.6 / day):\n",
      "    - 52 are answered (47.71%)\n",
      "    - 21 are dismissed (19.27%)\n",
      "    - response rate: 71.23%\n",
      "  Expectation of total delivered notifications is 2206.30\n",
      "\n",
      "===== end of week 19 ====\n",
      "  reward=-88.000000 / step=5040 (-0.017460)\n",
      "  106 notifications have been sent (15.1 / day):\n",
      "    - 47 are answered (44.34%)\n",
      "    - 27 are dismissed (25.47%)\n",
      "    - response rate: 63.51%\n",
      "  Expectation of total delivered notifications is 2275.30\n"
     ]
    }
   ],
   "source": [
    "numTotalWeeks = simulationLengthDay // 7\n",
    "for i in range(numTotalWeeks):\n",
    "    print()\n",
    "    print(\"===== end of week %d ====\" % i)\n",
    "    _printResults(_filterByWeek(simulationResults, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in simulationResults:\n",
    "    print(r['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/jp_test4/-100800'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.save_model('/tmp/jp_test4/')\n",
    "#print(agent.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.last_send_notification = datetime.datetime.now()\n",
    "agent.num_steps = 0\n",
    "agent.on_pickle_save()\n",
    "dill.dump(agent, open('../Nurture/server/notification/local_data/models/initial/tf-dqn-tuned.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf-dqn] generate_initial_model()\n",
      "[tf-dqn] _get_native_agent()\n",
      "[tf-dqn] done importing\n",
      "[tf-dqn] figured out number of dimensions 15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[tf-dqn] spawn the agent\n",
      "[tf-dqn] generate_initial_model() done\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/jp_test2/-100800\n",
      "[tf-dqn] on_pickle_save()\n",
      "[tf-dqn] on_pickle_save() done\n"
     ]
    }
   ],
   "source": [
    "agent2 = TensorForceDQNAgent()\n",
    "from tensorforce.agents import DQNAgent\n",
    "agent2.agent = DQNAgent(\n",
    "        states=dict(type='float', shape=(15,)),\n",
    "        actions=dict(type='int', num_actions=2),\n",
    "        network=[\n",
    "            dict(type='dense', size=20),\n",
    "            dict(type='dense', size=20)\n",
    "        ],\n",
    "        batched_observe=False,\n",
    "        actions_exploration={\n",
    "            'type': 'epsilon_decay',\n",
    "            'initial_epsilon': 0.1,\n",
    "            'final_epsilon': 0.015,\n",
    "            'timesteps': 3000,\n",
    "        },\n",
    ")\n",
    "agent2.agent.restore_model('/tmp/jp_test2/')\n",
    "agent2.on_pickle_save()\n",
    "dill.dump(agent2, open('../Nurture/server/notification/local_data/models/initial/tf-dqn-tuned.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDaysPassed, currentHour, currentMinute, currentDay = chronometer.forward(stepWidthMinutes)\n",
    "toBePrintedWeek = 0\n",
    "\n",
    "agent2 = TensorForceDQNAgent()\n",
    "agent2.agent = DQNAgent(\n",
    "        states=dict(type='float', shape=(15,)),\n",
    "        actions=dict(type='int', num_actions=2),\n",
    "        network=[\n",
    "            dict(type='dense', size=20),\n",
    "            dict(type='dense', size=20)\n",
    "        ],\n",
    "        batched_observe=False,\n",
    "        actions_exploration={\n",
    "            'type': 'epsilon_decay',\n",
    "            'initial_epsilon': 0.1,\n",
    "            'final_epsilon': 0.015,\n",
    "            'timesteps': 3000,\n",
    "        },\n",
    ")\n",
    "agent2.agent.restore_model('/tmp/jp_test/')\n",
    "#agent2.agent.model.network = agent.agent.model.network\n",
    "#agent2.agent.model.target_network = agent.agent.model.target_network\n",
    "#agent = QLearningAgent()\n",
    "#agent2 = agent\n",
    "\n",
    "while numDaysPassed < 21:\n",
    "    # get environment info (user context)\n",
    "    lastNotificationTime = utils.getDeltaMinutes(\n",
    "            numDaysPassed, currentHour, currentMinute,\n",
    "            lastNotificationNumDays, lastNotificationHour, lastNotificationMinute,\n",
    "    )\n",
    "    #stateLastNotification = utils.getLastNotificationState(lastNotificationTime)\n",
    "    stateLocation, stateActivity = behavior.getLocationActivity(\n",
    "            currentHour, currentMinute, currentDay)\n",
    "    probAnsweringNotification, probIgnoringNotification, probDismissingNotification = (\n",
    "            environment.getResponseDistribution(\n",
    "                currentHour, currentMinute, currentDay,\n",
    "                stateLocation, stateActivity, lastNotificationTime,\n",
    "            )\n",
    "    )\n",
    "    probAnsweringNotification, probIgnoringNotification, probDismissingNotification = utils.normalize(\n",
    "            probAnsweringNotification, probIgnoringNotification, probDismissingNotification)\n",
    "\n",
    "    # prepare observations\n",
    "    state = State(\n",
    "        timeOfDay=_get_time_of_day(currentHour, currentMinute),\n",
    "        dayOfWeek=_get_day_of_week(currentDay, currentHour, currentMinute),\n",
    "        motion=_get_motion(stateActivity),\n",
    "        location=stateLocation,\n",
    "        notificationTimeElapsed=lastNotificationTime,\n",
    "        ringerMode=np.random.choice(a=State.allRingerModeValues()),\n",
    "        screenStatus=np.random.choice(a=State.allScreenStatusValues()),\n",
    "    )\n",
    "    \n",
    "    # small hack - some agent keeps track of time\n",
    "    try:\n",
    "        agent2.last_notification_time -= datetime.timedelta(minutes=stepWidthMinutes)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get action\n",
    "    sendNotification = agent2.get_action(state)\n",
    "\n",
    "    # calculate reward\n",
    "    if not sendNotification:\n",
    "        reward = 0\n",
    "    else:\n",
    "        userReaction = np.random.choice(\n",
    "                a=[ANSWER_NOTIFICATION_ACCEPT, ANSWER_NOTIFICATION_IGNORE, ANSWER_NOTIFICATION_DISMISS],\n",
    "                p=[probAnsweringNotification, probIgnoringNotification, probDismissingNotification],\n",
    "        )\n",
    "        reward = rewardCriteria[userReaction]\n",
    "        lastNotificationNumDays = numDaysPassed\n",
    "        lastNotificationHour = currentHour\n",
    "        lastNotificationMinute = currentMinute\n",
    "    agent2.feed_reward(reward)\n",
    "\n",
    "    # log this session\n",
    "    simulationResults.append({\n",
    "            'context': {\n",
    "                'numDaysPassed': numDaysPassed,\n",
    "                'hour': currentHour,\n",
    "                'minute': currentMinute,\n",
    "                'day': currentDay,\n",
    "                'location': stateLocation,\n",
    "                'activity': stateActivity,\n",
    "                'lastNotification': lastNotificationTime,\n",
    "            },\n",
    "            'probOfAnswering': probAnsweringNotification,\n",
    "            'probOfIgnoring': probIgnoringNotification,\n",
    "            'probOfDismissing': probDismissingNotification,\n",
    "            'decision': sendNotification,\n",
    "            'reward': reward,\n",
    "    })\n",
    "\n",
    "    # get the next decision time point\n",
    "    numDaysPassed, currentHour, currentMinute, currentDay = chronometer.forward(stepWidthMinutes)\n",
    "    \n",
    "    # print current state\n",
    "    currentWeek = numDaysPassed // 7\n",
    "    if currentWeek > toBePrintedWeek:\n",
    "        print()\n",
    "        print(\"===== end of week %d ====\" % toBePrintedWeek)\n",
    "        _printResults(_filterByWeek(simulationResults, toBePrintedWeek))\n",
    "        toBePrintedWeek = currentWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== end of week 0 ====\n",
      "  reward=-815.000000 / step=5040 (-0.161706)\n",
      "  838 notifications have been sent (119.7 / day):\n",
      "    - 300 are answered (35.80%)\n",
      "    - 223 are dismissed (26.61%)\n",
      "    - response rate: 57.36%\n",
      "  Expectation of total delivered notifications is 1794.70\n",
      "\n",
      "===== end of week 1 ====\n",
      "  reward=-822.000000 / step=5040 (-0.163095)\n",
      "  840 notifications have been sent (120.0 / day):\n",
      "    - 298 are answered (35.48%)\n",
      "    - 224 are dismissed (26.67%)\n",
      "    - response rate: 57.09%\n",
      "  Expectation of total delivered notifications is 1766.70\n",
      "\n",
      "===== end of week 2 ====\n",
      "  reward=-739.000000 / step=5040 (-0.146627)\n",
      "  853 notifications have been sent (121.9 / day):\n",
      "    - 326 are answered (38.22%)\n",
      "    - 213 are dismissed (24.97%)\n",
      "    - response rate: 60.48%\n",
      "  Expectation of total delivered notifications is 1827.70\n"
     ]
    }
   ],
   "source": [
    "numTotalWeeks = simulationLengthDay // 7\n",
    "for i in range(3):\n",
    "    print()\n",
    "    print(\"===== end of week %d ====\" % i)\n",
    "    _printResults(_filterByWeek(simulationResults, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distribution': <tensorforce.core.distributions.categorical.Categorical at 0x7fc611c07400>,\n",
       " 'distribution_action': <tensorforce.core.distributions.categorical.Categorical at 0x7fc611c07400>,\n",
       " 'network': <tensorforce.core.networks.network.LayeredNetwork at 0x7fc69c6e87f0>,\n",
       " 'target_distribution': <tensorforce.core.distributions.categorical.Categorical at 0x7fc611c07fd0>,\n",
       " 'target_distribution_action': <tensorforce.core.distributions.categorical.Categorical at 0x7fc611c07fd0>,\n",
       " 'target_network': <tensorforce.core.networks.network.LayeredNetwork at 0x7fc611c076a0>}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.model.get_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/test_new_model/-206639'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.agent.save_model('/tmp/test_new_model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
